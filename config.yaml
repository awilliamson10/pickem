vocab_size: 4096
hidden_size: 1024
intermediate_size: 4096
num_hidden_layers: 16
num_attention_heads: 16
num_key_value_heads: None
hidden_act: "silu"
max_position_embeddings: 1024
initializer_range: 0.02
rms_norm_eps: 1e-6
use_cache: True

pad_token_id: 4096
bos_token_id: 0
eos_token_id: 1

tie_word_embeddings: True

seed: 42
device: "cuda"
output_dir: "./pbp_output"
wandb: True
wandb_project: pbp
train_dataset: awilliamson/pbp_tokenized
eval_dataset: awilliamson/pbp_tokenized
compile: True
precision: "bf16"
batch_size: 32
gradient_accumulation_steps: 2
gradient_checkpointing: True
use_8bit_adam: False
learning_rate: 1e-4
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8
epochs: 5
warmup: 0.05
save_interval: 2000
eval_interval: 250
eval_steps: 100