vocab_size: 768
hidden_size: 2048
intermediate_size: 4096
num_hidden_layers: 24
num_attention_heads: 16

hidden_act: "silu"
max_position_embeddings: 1024
initializer_range: 0.02
use_cache: False

pad_token_id: 1
bos_token_id: 5
eos_token_id: 6

tie_word_embeddings: True

seed: 42
device: "cuda"
output_dir: "./pickem"
wandb: True
wandb_project: pickem-team-emb
train_dataset: awilliamson/pickem_tokenized-te
eval_dataset: awilliamson/pickem_tokenized-te
compile: False
precision: "bf16"
batch_size: 64
gradient_accumulation_steps: 1
gradient_checkpointing: True
use_8bit_adam: False
learning_rate: 1e-4
adam_beta1: 0.9
adam_beta2: 0.999
epochs: 3
warmup: 0.01
save_interval: 500
eval_interval: 100
eval_steps: 1